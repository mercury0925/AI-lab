{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercury0925/AI-lab/blob/main/AIweek4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Feature Extractions"
      ],
      "metadata": {
        "id": "HetE68HNmwuD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7P9nYG8e6_K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation**"
      ],
      "metadata": {
        "id": "5DT_Fp_Kg5v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "id": "D17POY5wfn4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "93aWRkXSgR1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "from matplotlib.patches import Rectangle\n",
        "from torchaudio.utils import download_asset\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/englishsentence.m4a\"\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "\n",
        "def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sr\n",
        "\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(num_channels, 1)\n",
        "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
        "    ax.grid(True)\n",
        "    ax.set_xlim([0, time_axis[-1]])\n",
        "    ax.set_title(title)\n",
        "\n",
        "\n",
        "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(1, 1)\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    im = ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
        "\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label('Power (dB)', rotation=270, labelpad=15)\n",
        "\n",
        "def plot_fbank(fbank, title=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or \"Filter bank\")\n",
        "    axs.imshow(fbank, aspect=\"auto\")\n",
        "    axs.set_ylabel(\"frequency bin\")\n",
        "    axs.set_xlabel(\"mel bin\")"
      ],
      "metadata": {
        "id": "gm7F80FcfxQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectrogram**"
      ],
      "metadata": {
        "id": "oMvqdER6g9N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio.transforms as T\n",
        "# Define transform\n",
        "spectrogram_transform = T.Spectrogram(n_fft=512)\n",
        "# Perform transform\n",
        "spec = spectrogram_transform(waveform)"
      ],
      "metadata": {
        "id": "42zVD_qag--k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 1)\n",
        "plot_waveform(waveform, sample_rate, title=\"Original waveform\", ax=axs[0])\n",
        "plot_spectrogram(spec[0], title=\"Spectrogram\", ax=axs[1])\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "O99vcxsqhTfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "metadata": {
        "id": "FQE5fSjJhud6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effect of n_fft parameter"
      ],
      "metadata": {
        "id": "fkkZGdRSh1XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_ffts = [32, 128, 512, 2048]\n",
        "hop_length = 64\n",
        "\n",
        "specs = []\n",
        "for n_fft in n_ffts:\n",
        "    spectrogram = T.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
        "    spec = spectrogram(waveform)\n",
        "    specs.append(spec)"
      ],
      "metadata": {
        "id": "N6LRi6VMh2iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(len(specs), 1, sharex=True)\n",
        "for i, (spec, n_fft) in enumerate(zip(specs, n_ffts)):\n",
        "    plot_spectrogram(spec[0], ylabel=f\"n_fft={n_fft}\", ax=axs[i])\n",
        "    axs[i].set_xlabel(None)\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "6Oh69en6h6TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsample to half of the original sample rate\n",
        "speech2 = torchaudio.functional.resample(waveform, sample_rate, sample_rate // 2)\n",
        "# Upsample to the original sample rate\n",
        "speech3 = torchaudio.functional.resample(speech2, sample_rate // 2, sample_rate)"
      ],
      "metadata": {
        "id": "UwbvhdKSiQKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the same spectrogram\n",
        "spectrogram = T.Spectrogram(n_fft=512)\n",
        "\n",
        "spec0 = spectrogram(waveform)\n",
        "spec2 = spectrogram(speech2)\n",
        "spec3 = spectrogram(speech3)"
      ],
      "metadata": {
        "id": "uKAxB3kuiSzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize it\n",
        "fig, axs = plt.subplots(3, 1)\n",
        "plot_spectrogram(spec0[0], ylabel=\"Original\", ax=axs[0])\n",
        "axs[0].add_patch(Rectangle((0, 3), 212, 128, edgecolor=\"r\", facecolor=\"none\"))\n",
        "plot_spectrogram(spec2[0], ylabel=\"Downsampled\", ax=axs[1])\n",
        "plot_spectrogram(spec3[0], ylabel=\"Upsampled\", ax=axs[2])\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "EVZHfniQiU1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GriffinLim**"
      ],
      "metadata": {
        "id": "0zXj6bpRiYLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "n_fft = 1024\n",
        "spectrogram = T.Spectrogram(n_fft=n_fft)\n",
        "griffin_lim = T.GriffinLim(n_fft=n_fft)\n",
        "\n",
        "# Apply the transforms\n",
        "spec = spectrogram(waveform)\n",
        "reconstructed_waveform = griffin_lim(spec)"
      ],
      "metadata": {
        "id": "cQs1R2pdiY2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
        "plot_waveform(waveform, sample_rate, title=\"Original\", ax=axes[0])\n",
        "plot_waveform(reconstructed_waveform, sample_rate, title=\"Reconstructed\", ax=axes[1])\n",
        "#Audio(reconstructed_waveform, rate=sample_rate)"
      ],
      "metadata": {
        "id": "TbJnLlcPicMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mel Filter Bank"
      ],
      "metadata": {
        "id": "pTNEvGWfii2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_fft = 256\n",
        "n_mels = 64\n",
        "sample_rate = 6000\n",
        "\n",
        "mel_filters = F.melscale_fbanks(\n",
        "    int(n_fft // 2 + 1),\n",
        "    n_mels=n_mels,\n",
        "    f_min=0.0,\n",
        "    f_max=sample_rate / 2.0,\n",
        "    sample_rate=sample_rate,\n",
        "    norm=\"slaney\",\n",
        ")"
      ],
      "metadata": {
        "id": "IZfUTPUTim_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_fbank(mel_filters, \"Mel Filter Bank - torchaudio\")"
      ],
      "metadata": {
        "id": "IKk7HGlDiul-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison against librosa"
      ],
      "metadata": {
        "id": "gEOZMKOeixia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mel_filters_librosa = librosa.filters.mel(\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    n_mels=n_mels,\n",
        "    fmin=0.0,\n",
        "    fmax=sample_rate / 2.0,\n",
        "    norm=\"slaney\",\n",
        "    htk=True,\n",
        ").T"
      ],
      "metadata": {
        "id": "uPQ8o3bKizGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_fbank(mel_filters_librosa, \"Mel Filter Bank - librosa\")\n",
        "\n",
        "mse = torch.square(mel_filters - mel_filters_librosa).mean().item()\n",
        "print(\"Mean Square Difference: \", mse)"
      ],
      "metadata": {
        "id": "jAxtBk5ji29S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MelSpectrogram"
      ],
      "metadata": {
        "id": "gLhQzQVmjewZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_fft = 1024\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "\n",
        "mel_spectrogram = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    norm=\"slaney\",\n",
        "    n_mels=n_mels,\n",
        "    mel_scale=\"htk\",\n",
        ")\n",
        "\n",
        "melspec = mel_spectrogram(waveform)"
      ],
      "metadata": {
        "id": "1uyWVuAmjfjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectrogram(melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
      ],
      "metadata": {
        "id": "p1oQlkx_jom0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison against librosa"
      ],
      "metadata": {
        "id": "uggIG_E5j-gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "melspec_librosa = librosa.feature.melspectrogram(\n",
        "    y=waveform.numpy()[0],\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    hop_length=hop_length,\n",
        "    win_length=win_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    n_mels=n_mels,\n",
        "    norm=\"slaney\",\n",
        "    htk=True,\n",
        ")"
      ],
      "metadata": {
        "id": "vCTqQFUyj_9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectrogram(melspec_librosa, title=\"MelSpectrogram - librosa\", ylabel=\"mel freq\")\n",
        "\n",
        "mse = torch.square(melspec - melspec_librosa).mean().item()\n",
        "print(\"Mean Square Difference: \", mse)"
      ],
      "metadata": {
        "id": "ilOPDzyqkLUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MFCC"
      ],
      "metadata": {
        "id": "MLMGzHAvkNzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 256\n",
        "n_mfcc = 256\n",
        "\n",
        "mfcc_transform = T.MFCC(\n",
        "    sample_rate=sample_rate,\n",
        "    n_mfcc=n_mfcc,\n",
        "    melkwargs={\n",
        "        \"n_fft\": n_fft,\n",
        "        \"n_mels\": n_mels,\n",
        "        \"hop_length\": hop_length,\n",
        "        \"mel_scale\": \"htk\",\n",
        "    },\n",
        ")\n",
        "\n",
        "mfcc = mfcc_transform(waveform)"
      ],
      "metadata": {
        "id": "SouQqcCPkOcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectrogram(mfcc[0], title=\"MFCC\")"
      ],
      "metadata": {
        "id": "h5ySHteykTvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison against librosa"
      ],
      "metadata": {
        "id": "kvJRMKbbkaU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "melspec = librosa.feature.melspectrogram(\n",
        "    y=waveform.numpy()[0],\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    n_mels=n_mels,\n",
        "    htk=True,\n",
        "    norm=None,\n",
        ")\n",
        "\n",
        "mfcc_librosa = librosa.feature.mfcc(\n",
        "    S=librosa.core.spectrum.power_to_db(melspec),\n",
        "    n_mfcc=n_mfcc,\n",
        "    dct_type=2,\n",
        "    norm=\"ortho\",\n",
        ")"
      ],
      "metadata": {
        "id": "vUVaYkMBka7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectrogram(mfcc_librosa, title=\"MFCC (librosa)\")\n",
        "\n",
        "mse = torch.square(mfcc - mfcc_librosa).mean().item()\n",
        "print(\"Mean Square Difference: \", mse)"
      ],
      "metadata": {
        "id": "JoLBCNEykhj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LFCC"
      ],
      "metadata": {
        "id": "lVJds4qhkkW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_lfcc = 256\n",
        "\n",
        "lfcc_transform = T.LFCC(\n",
        "    sample_rate=sample_rate,\n",
        "    n_lfcc=n_lfcc,\n",
        "    speckwargs={\n",
        "        \"n_fft\": n_fft,\n",
        "        \"win_length\": win_length,\n",
        "        \"hop_length\": hop_length,\n",
        "    },\n",
        ")\n",
        "\n",
        "lfcc = lfcc_transform(waveform)\n",
        "plot_spectrogram(lfcc[0], title=\"LFCC\")"
      ],
      "metadata": {
        "id": "zfF5Ol1wkk4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pitch"
      ],
      "metadata": {
        "id": "XaSN9ye6ko_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch = F.detect_pitch_frequency(waveform, sample_rate)"
      ],
      "metadata": {
        "id": "tkmY4P7mkpkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pitch(waveform, sr, pitch):\n",
        "    figure, axis = plt.subplots(1, 1)\n",
        "    axis.set_title(\"Pitch Feature\")\n",
        "    axis.grid(True)\n",
        "\n",
        "    end_time = waveform.shape[1] / sr\n",
        "    time_axis = torch.linspace(0, end_time, waveform.shape[1])\n",
        "    axis.plot(time_axis, waveform[0], linewidth=1, color=\"gray\", alpha=0.3)\n",
        "\n",
        "    axis2 = axis.twinx()\n",
        "    time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "    axis2.plot(time_axis, pitch[0], linewidth=2, label=\"Pitch\", color=\"green\")\n",
        "\n",
        "    axis2.legend(loc=0)\n",
        "\n",
        "\n",
        "plot_pitch(waveform, sample_rate, pitch)"
      ],
      "metadata": {
        "id": "bE7pLQuzkssk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech Recognition with Wav2Vec2"
      ],
      "metadata": {
        "id": "R_KTXLWkmrhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "9mHYGAQcmngQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SPEECH_FILE = \"/content/drive/MyDrive/englishsentence.m4a\""
      ],
      "metadata": {
        "id": "9rMkPK4ym4OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "\n",
        "print(\"Sample Rate:\", bundle.sample_rate)\n",
        "\n",
        "print(\"Labels:\", bundle.get_labels())"
      ],
      "metadata": {
        "id": "jyhELtjuoIWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = bundle.get_model().to(device)\n",
        "\n",
        "print(model.__class__)"
      ],
      "metadata": {
        "id": "8UfILll6oPXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IPython.display.Audio(SPEECH_FILE)"
      ],
      "metadata": {
        "id": "sXbMs41FoTVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "waveform = waveform.to(device)\n",
        "\n",
        "if sample_rate != bundle.sample_rate:\n",
        "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
      ],
      "metadata": {
        "id": "I0ltqPFjoZG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    features, _ = model.extract_features(waveform)"
      ],
      "metadata": {
        "id": "3qw7VCrmotHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(len(features), 1, figsize=(16, 4.3 * len(features)))\n",
        "for i, feats in enumerate(features):\n",
        "    ax[i].imshow(feats[0].cpu(), interpolation=\"nearest\")\n",
        "    ax[i].set_title(f\"Feature from transformer layer {i+1}\")\n",
        "    ax[i].set_xlabel(\"Feature dimension\")\n",
        "    ax[i].set_ylabel(\"Frame (time-axis)\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "_PjwMD_3owE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    emission, _ = model(waveform)"
      ],
      "metadata": {
        "id": "spwQ6xGApBlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(emission[0].cpu().T, interpolation=\"nearest\")\n",
        "plt.title(\"Classification result\")\n",
        "plt.xlabel(\"Frame (time-axis)\")\n",
        "plt.ylabel(\"Class\")\n",
        "plt.tight_layout()\n",
        "print(\"Class labels:\", bundle.get_labels())"
      ],
      "metadata": {
        "id": "Lcuv5B6KpDkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyCTCDecoder(torch.nn.Module):\n",
        "    def __init__(self, labels, blank=0):\n",
        "        super().__init__()\n",
        "        self.labels = labels\n",
        "        self.blank = blank\n",
        "\n",
        "    def forward(self, emission: torch.Tensor) -> str:\n",
        "        \"\"\"Given a sequence emission over labels, get the best path string\n",
        "        Args:\n",
        "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
        "\n",
        "        Returns:\n",
        "          str: The resulting transcript\n",
        "        \"\"\"\n",
        "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
        "        indices = torch.unique_consecutive(indices, dim=-1)\n",
        "        indices = [i for i in indices if i != self.blank]\n",
        "        return \"\".join([self.labels[i] for i in indices])"
      ],
      "metadata": {
        "id": "Tp1sAj4Zpg7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
        "transcript = decoder(emission[0])"
      ],
      "metadata": {
        "id": "RID6s7Uapj-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcript)\n",
        "#IPython.display.Audio(SPEECH_FILE)"
      ],
      "metadata": {
        "id": "iX6XDFogpnDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-pyctcdecode를 활용해 Beam Search decoding으로 결과 정확도 개선"
      ],
      "metadata": {
        "id": "ZzIBhwvhiU13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 transformers datasets pyctcdecode\n",
        "!pip install git+https://github.com/kensho-technologies/pyctcdecode.git\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from pyctcdecode import build_ctcdecoder\n",
        "\n",
        "model_id = \"facebook/wav2vec2-base-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id).to(device)\n",
        "\n",
        "input_values = processor(waveform.squeeze().cpu().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "vocab = processor.tokenizer.get_vocab()\n",
        "sorted_vocab = [k for k,v in sorted(vocab.items(), key=lambda item: item[1])]\n",
        "decoder = build_ctcdecoder(sorted_vocab)\n",
        "\n",
        "beam_result = decoder.decode(logits.cpu().numpy()[0])\n",
        "print(\"Beam Search Result:\", beam_result)"
      ],
      "metadata": {
        "id": "DJ-MpcnkeG7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "오디오 전처리(정규화 + 리샘플링 + 무음제거) -> Wav2Vec2 모델을 사용해 Greedy decoding"
      ],
      "metadata": {
        "id": "7-4huiQ4iXx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "\n",
        "waveform = waveform / waveform.abs().max()\n",
        "\n",
        "if sample_rate != 16000:\n",
        "    waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "    sample_rate = 16000\n",
        "\n",
        "waveform = torchaudio.functional.vad(waveform, sample_rate=sample_rate)\n",
        "\n",
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "model = bundle.get_model().to(device)\n",
        "labels = bundle.get_labels()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    emission, _ = model(waveform.to(device))\n",
        "\n",
        "class GreedyCTCDecoder(torch.nn.Module):\n",
        "    def __init__(self, labels, blank=0):\n",
        "        super().__init__()\n",
        "        self.labels = labels\n",
        "        self.blank = blank\n",
        "    def forward(self, emission: torch.Tensor) -> str:\n",
        "        indices = torch.argmax(emission, dim=-1)\n",
        "        indices = torch.unique_consecutive(indices, dim=-1)\n",
        "        indices = [i for i in indices if i != self.blank]\n",
        "        return \"\".join([self.labels[i] for i in indices])\n",
        "\n",
        "decoder = GreedyCTCDecoder(labels)\n",
        "transcript = decoder(emission[0])\n",
        "print(\"Greedy + 전처리 Transcript:\", transcript.replace(\"|\",\" \"))\n"
      ],
      "metadata": {
        "id": "HEOKejpWhjco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-HuggingFace의 pipeline을 이용, OpenAI Whisper 모델(whisper-small.en)"
      ],
      "metadata": {
        "id": "dtUkzYwoixbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small.en\")\n",
        "\n",
        "result = asr(SPEECH_FILE)\n",
        "print(\"Whisper Transcript:\", result[\"text\"])"
      ],
      "metadata": {
        "id": "m4Gw62mWh365"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}